{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8ZyJgNAlV7n",
        "outputId": "f60d4922-d0be-4e21-f62d-11979e141bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/453.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m450.6/453.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langgraph langchain-core langchain-anthropic anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: load API key from Colab Secrets (recommended)\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "CANDIDATES = [\"ANTHROPIC_API_KEY\", \"ANTHROPIC\", \"ANTHROPIC_KEY\"]\n",
        "key = None\n",
        "for name in CANDIDATES:\n",
        "    try:\n",
        "        v = userdata.get(name)\n",
        "        if v:\n",
        "            key = v\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not key:\n",
        "    raise RuntimeError(\n",
        "        \"Missing Anthropic API key. Add a Colab Secret named ANTHROPIC_API_KEY (or ANTHROPIC).\"\n",
        "    )\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = key\n",
        "print(\"Loaded ANTHROPIC_API_KEY from Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNQIhi0lmtf",
        "outputId": "cf8e7f67-69d0-4162-85f1-9e5d8045d315"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded ANTHROPIC_API_KEY from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal, Optional\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "\n",
        "class AppState(TypedDict):\n",
        "    user_input: str\n",
        "    route: Optional[Literal[\"math\", \"writing\", \"general\"]]\n",
        "    draft: Optional[str]\n",
        "    critique: Optional[str]\n",
        "    tries: int\n",
        "    final: Optional[str]\n",
        "\n",
        "\n",
        "llm = ChatAnthropic(\n",
        "    model=\"claude-sonnet-4-6\",\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "\n",
        "def router_node(state: AppState) -> AppState:\n",
        "    msgs = [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"Classify the user's request into exactly one label:\\n\"\n",
        "                \"- math: calculations, quantitative reasoning, formulas\\n\"\n",
        "                \"- writing: rewriting, polishing, summarizing, formatting text\\n\"\n",
        "                \"- general: anything else\\n\"\n",
        "                \"Return ONLY the label.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(content=state[\"user_input\"]),\n",
        "    ]\n",
        "    resp = llm.invoke(msgs).content.strip().lower()\n",
        "    route: Literal[\"math\", \"writing\", \"general\"] = \"general\"\n",
        "    if \"math\" in resp:\n",
        "        route = \"math\"\n",
        "    elif \"writing\" in resp:\n",
        "        route = \"writing\"\n",
        "    return {**state, \"route\": route}\n",
        "\n",
        "\n",
        "def math_node(state: AppState) -> AppState:\n",
        "    critique = state.get(\"critique\") or \"\"\n",
        "    msgs = [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a careful math tutor.\\n\"\n",
        "                \"Produce a clear solution with:\\n\"\n",
        "                \"1) Given/Goal\\n\"\n",
        "                \"2) Step-by-step reasoning\\n\"\n",
        "                \"3) Final answer\\n\"\n",
        "                \"If critique is provided, fix those issues.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=f\"User request:\\n{state['user_input']}\\n\\nCritique to address:\\n{critique}\"\n",
        "        ),\n",
        "    ]\n",
        "    draft = llm.invoke(msgs).content\n",
        "    return {**state, \"draft\": draft}\n",
        "\n",
        "\n",
        "def writing_node(state: AppState) -> AppState:\n",
        "    critique = state.get(\"critique\") or \"\"\n",
        "    msgs = [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a writing assistant.\\n\"\n",
        "                \"Follow the user's formatting requirements precisely.\\n\"\n",
        "                \"If critique is provided, fix those issues.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=f\"User request:\\n{state['user_input']}\\n\\nCritique to address:\\n{critique}\"\n",
        "        ),\n",
        "    ]\n",
        "    draft = llm.invoke(msgs).content\n",
        "    return {**state, \"draft\": draft}\n",
        "\n",
        "\n",
        "def general_node(state: AppState) -> AppState:\n",
        "    critique = state.get(\"critique\") or \"\"\n",
        "    msgs = [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant.\\n\"\n",
        "                \"Be accurate and follow the user's constraints.\\n\"\n",
        "                \"If critique is provided, fix those issues.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=f\"User request:\\n{state['user_input']}\\n\\nCritique to address:\\n{critique}\"\n",
        "        ),\n",
        "    ]\n",
        "    draft = llm.invoke(msgs).content\n",
        "    return {**state, \"draft\": draft}\n",
        "\n",
        "\n",
        "def checker_node(state: AppState) -> AppState:\n",
        "    draft = state.get(\"draft\") or \"\"\n",
        "    msgs = [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a strict QA checker.\\n\"\n",
        "                \"Decide whether the draft fully satisfies the user request.\\n\"\n",
        "                \"Return JSON with keys:\\n\"\n",
        "                '{ \"verdict\": \"ok\"|\"revise\", \"critique\": \"...\" }\\n'\n",
        "                \"Keep critique short and actionable.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessage(content=f\"User request:\\n{state['user_input']}\\n\\nDraft:\\n{draft}\"),\n",
        "    ]\n",
        "    resp = llm.invoke(msgs).content.strip().lower()\n",
        "\n",
        "    verdict = \"revise\"\n",
        "    if '\"verdict\"' in resp and '\"ok\"' in resp:\n",
        "        verdict = \"ok\"\n",
        "    elif resp.strip() == \"ok\":\n",
        "        verdict = \"ok\"\n",
        "\n",
        "    critique = \"\"\n",
        "    if \"critique\" in resp:\n",
        "        critique = resp.split(\"critique\", 1)[-1].strip()[:500]\n",
        "\n",
        "    tries = state.get(\"tries\", 0) + 1\n",
        "\n",
        "    if verdict == \"ok\" or tries >= 2:\n",
        "        return {**state, \"final\": draft, \"tries\": tries, \"critique\": critique}\n",
        "    else:\n",
        "        return {**state, \"tries\": tries, \"critique\": critique, \"final\": None}\n",
        "\n",
        "\n",
        "def route_selector(state: AppState) -> str:\n",
        "    return state[\"route\"] or \"general\"\n",
        "\n",
        "\n",
        "def check_selector(state: AppState) -> str:\n",
        "    if state.get(\"final\"):\n",
        "        return \"end\"\n",
        "    return state[\"route\"] or \"general\"\n",
        "\n",
        "\n",
        "graph = StateGraph(AppState)\n",
        "\n",
        "graph.add_node(\"router\", router_node)\n",
        "graph.add_node(\"math\", math_node)\n",
        "graph.add_node(\"writing\", writing_node)\n",
        "graph.add_node(\"general\", general_node)\n",
        "graph.add_node(\"checker\", checker_node)\n",
        "\n",
        "graph.add_edge(START, \"router\")\n",
        "graph.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_selector,\n",
        "    {\"math\": \"math\", \"writing\": \"writing\", \"general\": \"general\"},\n",
        ")\n",
        "\n",
        "graph.add_edge(\"math\", \"checker\")\n",
        "graph.add_edge(\"writing\", \"checker\")\n",
        "graph.add_edge(\"general\", \"checker\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"checker\",\n",
        "    check_selector,\n",
        "    {\"math\": \"math\", \"writing\": \"writing\", \"general\": \"general\", \"end\": END},\n",
        ")\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "\n",
        "def run_once(user_input: str) -> str:\n",
        "    init: AppState = {\n",
        "        \"user_input\": user_input,\n",
        "        \"route\": None,\n",
        "        \"draft\": None,\n",
        "        \"critique\": None,\n",
        "        \"tries\": 0,\n",
        "        \"final\": None,\n",
        "    }\n",
        "    out = app.invoke(init)\n",
        "    return out.get(\"final\") or \"(no output)\""
      ],
      "metadata": {
        "id": "T8HswrE20GRj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: run a quick demo\n",
        "print(run_once(\"Explain LangGraph in 5 bullet points.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rERswE2c0K6H",
        "outputId": "6e5c2df4-fb04-4b34-ce43-e8789a3cb111"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 bullet points explaining LangGraph:\n",
            "\n",
            "‚Ä¢ **Graph-Based Framework** ‚Äì LangGraph is a library built on top of LangChain that allows developers to build stateful, multi-step AI applications by modeling workflows as graphs, where nodes represent actions or computations and edges define the flow between them.\n",
            "\n",
            "‚Ä¢ **Stateful Execution** ‚Äì Unlike simple linear chains, LangGraph maintains a shared state across all nodes in the graph, allowing information to persist and be updated as the workflow progresses through different steps.\n",
            "\n",
            "‚Ä¢ **Cycles and Loops Support** ‚Äì One of LangGraph's key strengths is its ability to handle cyclic graphs, enabling loops and conditional branching, which is essential for building agentic systems that need to retry, reflect, or iterate on tasks.\n",
            "\n",
            "‚Ä¢ **Agent and Multi-Agent Workflows** ‚Äì LangGraph is particularly well-suited for building autonomous AI agents and complex multi-agent systems, where multiple AI models or tools can collaborate, hand off tasks, and make decisions dynamically.\n",
            "\n",
            "‚Ä¢ **Human-in-the-Loop & Controllability** ‚Äì LangGraph supports human-in-the-loop interactions, allowing developers to pause execution, inject human feedback, and resume workflows, making it easier to build reliable and controllable AI applications in production environments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: try more prompts\n",
        "tests = [\n",
        "    \"Rewrite this in a professional tone: I messed up the report and I need more time.\",\n",
        "    \"Solve: If x + y = 10 and x - y = 2, find x and y.\",\n",
        "    \"Give me a concise checklist for preparing for a coding interview.\",\n",
        "]\n",
        "for t in tests:\n",
        "    print(\"\\n---\\nPrompt:\", t)\n",
        "    print(run_once(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3oLPocw0p1l",
        "outputId": "57b16cc7-5ca1-4880-fb8d-ec3e49d14747"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Prompt: Rewrite this in a professional tone: I messed up the report and I need more time.\n",
            "\"I apologize for the oversight in the report and would like to request an extension to ensure the final submission meets the required standards.\"\n",
            "\n",
            "---\n",
            "Prompt: Solve: If x + y = 10 and x - y = 2, find x and y.\n",
            "# Solving the System of Equations\n",
            "\n",
            "## Given Equations\n",
            "$$x + y = 10 \\quad \\text{...(1)}$$\n",
            "$$x - y = 2 \\quad \\text{...(2)}$$\n",
            "\n",
            "## Solution\n",
            "\n",
            "**Step 1: Add both equations together**\n",
            "\n",
            "$$( x + y) + (x - y) = 10 + 2$$\n",
            "\n",
            "$$2x = 12$$\n",
            "\n",
            "$$x = 6$$\n",
            "\n",
            "**Step 2: Substitute x = 6 into equation (1)**\n",
            "\n",
            "$$6 + y = 10$$\n",
            "\n",
            "$$y = 10 - 6$$\n",
            "\n",
            "$$y = 4$$\n",
            "\n",
            "## Answer\n",
            "\n",
            "$$\\boxed{x = 6, \\quad y = 4}$$\n",
            "\n",
            "## Verification\n",
            "| Equation | Check |\n",
            "|----------|-------|\n",
            "| x + y = 10 | 6 + 4 = 10 ‚úÖ |\n",
            "| x - y = 2 | 6 - 4 = 2 ‚úÖ |\n",
            "\n",
            "---\n",
            "Prompt: Give me a concise checklist for preparing for a coding interview.\n",
            "# Coding Interview Preparation Checklist\n",
            "\n",
            "## üìö Data Structures\n",
            "- [ ] Arrays & Strings\n",
            "- [ ] Linked Lists\n",
            "- [ ] Stacks & Queues\n",
            "- [ ] Hash Maps / Hash Sets\n",
            "- [ ] Trees & Binary Search Trees\n",
            "- [ ] Graphs\n",
            "- [ ] Heaps\n",
            "\n",
            "## üß† Algorithms\n",
            "- [ ] Sorting (Merge, Quick, Heap)\n",
            "- [ ] Binary Search\n",
            "- [ ] BFS & DFS\n",
            "- [ ] Dynamic Programming\n",
            "- [ ] Recursion & Backtracking\n",
            "- [ ] Two Pointers & Sliding Window\n",
            "\n",
            "## üíª Practice\n",
            "- [ ] Solve 50‚Äì100 LeetCode problems (Easy ‚Üí Medium ‚Üí Hard)\n",
            "- [ ] Practice problems under timed conditions\n",
            "- [ ] Review and understand optimal solutions\n",
            "- [ ] Practice on a whiteboard or plain editor\n",
            "\n",
            "## üó£Ô∏è Communication\n",
            "- [ ] Think out loud while solving\n",
            "- [ ] Clarify requirements before coding\n",
            "- [ ] Discuss trade-offs (time vs. space complexity)\n",
            "- [ ] Walk through examples and edge cases\n",
            "\n",
            "## üîç Before the Interview\n",
            "- [ ] Research the company and its tech stack\n",
            "- [ ] Review your past projects and resume\n",
            "- [ ] Prepare questions to ask the interviewer\n",
            "- [ ] Test your setup (for virtual interviews)\n",
            "\n",
            "## ‚úÖ During the Interview\n",
            "- [ ] Start with brute force, then optimize\n",
            "- [ ] State time & space complexity\n",
            "- [ ] Test your solution with edge cases\n",
            "- [ ] Stay calm and composed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "client = anthropic.Anthropic()\n",
        "models = client.models.list()\n",
        "print([m.id for m in models.data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl24dhL80uUR",
        "outputId": "21c4d9c8-3b8d-43e8-e5ae-5025e10193c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['claude-sonnet-4-6', 'claude-opus-4-6', 'claude-opus-4-5-20251101', 'claude-haiku-4-5-20251001', 'claude-sonnet-4-5-20250929', 'claude-opus-4-1-20250805', 'claude-opus-4-20250514', 'claude-sonnet-4-20250514', 'claude-3-7-sonnet-20250219', 'claude-3-5-haiku-20241022', 'claude-3-haiku-20240307']\n"
          ]
        }
      ]
    }
  ]
}